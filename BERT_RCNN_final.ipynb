{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_RCNN_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e01cfb91a2f94a3ab66a0c5b0d4f965b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d27329495704389ac07370cef39f4b4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_712e8d1ea47546ccb4f2e471701f69c0",
              "IPY_MODEL_316ea6cd3065459d861c63668872fb35"
            ]
          }
        },
        "6d27329495704389ac07370cef39f4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "712e8d1ea47546ccb4f2e471701f69c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_52ee244311284f4fa4715d92b0d79b6e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_329f52a71c7e47a19324f8bb7b99b677"
          }
        },
        "316ea6cd3065459d861c63668872fb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04e9cca30b2747ff893829b53f3c0e72",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.42MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0709537a6644c1f9d3a78d7dca96175"
          }
        },
        "52ee244311284f4fa4715d92b0d79b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "329f52a71c7e47a19324f8bb7b99b677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04e9cca30b2747ff893829b53f3c0e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0709537a6644c1f9d3a78d7dca96175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f74421d08f748d9a3bae9ce387c0c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_818518f7363a4fd79be0c23d52f08b71",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c18dbb95c514e80901b92446c04c16d",
              "IPY_MODEL_9ef755b4f5744da0a1e3bdbbb6abad84"
            ]
          }
        },
        "818518f7363a4fd79be0c23d52f08b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c18dbb95c514e80901b92446c04c16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8473a95bd8cd44d1ac88d8e583c1a51a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72fe19108e974fbcaea52f547e17a249"
          }
        },
        "9ef755b4f5744da0a1e3bdbbb6abad84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_437b62d6119f462bb23c1fb0efce58ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 611B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41f4d469b53a4979a623223fd4ed57ba"
          }
        },
        "8473a95bd8cd44d1ac88d8e583c1a51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72fe19108e974fbcaea52f547e17a249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "437b62d6119f462bb23c1fb0efce58ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41f4d469b53a4979a623223fd4ed57ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b975a7ed3e554f6392314a65deff0f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f36fb454159342dfbcf05e8a9b17a82d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72e8dab49abe447c800db6fbbf3b87cc",
              "IPY_MODEL_f05e910647f649dab0f12e343da8ce38"
            ]
          }
        },
        "f36fb454159342dfbcf05e8a9b17a82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72e8dab49abe447c800db6fbbf3b87cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dcf0533513894c43bed4c507a603db71",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_419c974d4cc84d47bce7855daa72d442"
          }
        },
        "f05e910647f649dab0f12e343da8ce38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1bcd1d7f577543e5a09c3486ee9de212",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 62.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea4bebd2196b42fcab27912a7afebc37"
          }
        },
        "dcf0533513894c43bed4c507a603db71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "419c974d4cc84d47bce7855daa72d442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bcd1d7f577543e5a09c3486ee9de212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea4bebd2196b42fcab27912a7afebc37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl5Te8MLqiPc",
        "colab_type": "text"
      },
      "source": [
        "# About the notebook\n",
        "\n",
        "The following notebook contains the Bert model with Recurrent Convolutional Neural Network for the classification purpose (sentiment analysis & text classification). The notebook has a following order:\n",
        "\n",
        "1. [Data Import & Preprocess](#section_1)\n",
        "2. [Tokenization & Preprocess for model](#section_2)\n",
        "3. [Fine-Tuning Function](#section_3)\n",
        "4. [Fine-Tuning](#section_4)\n",
        "5. [Confusion matrix & Critical Error](#section_5)\n",
        "6. [Train & Test](#section_6)\n",
        "\n",
        "Please refer to the BERT_Linear_layer for the purpose of the note book.\n",
        "The original idea is from [source_code]((https://github.com/roomylee/rcnn-text-classification/blob/master/rcnn.py) and [article](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.822.3091&rep=rep1&type=pdf). Please follow the link for those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTi695V7w1gV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "a0df088c-945c-4e8c-9081-a8d5c2eb40bc"
      },
      "source": [
        "#===============================================================================\n",
        "# Import Libraries and Download module necessary for the deep learning\n",
        "#===============================================================================\n",
        "\n",
        "#huggingface library installation\n",
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import BertForTokenClassification, AdamW, BertConfig, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "import random\n",
        "import transformers\n",
        "from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import datetime\n",
        "from platform import python_version\n",
        "import sklearn\n",
        "import torch\n",
        "\n",
        "#Using Colab GPU for training\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "#To confirm that we are using GPU for the training later\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 32.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b044348d6a69fa06aae60363f3ee463a612bf3932919caef901f51b1c6d4f96b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIxdNrQwxTkY",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Import & Preprocess <a class=\"anchor\" id=\"section_1\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGl-TFpHxTAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6792d045-d932-4776-ec33-3984194b0f12"
      },
      "source": [
        "#===============================================================================\n",
        "# Load Google Drive for the data\n",
        "#===============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_direction = '/content/drive/My Drive/KIS data/CIMS_news_with_bertext_sum_full.xlsx'\n",
        "df = pd.read_excel(file_direction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE_WMrzsw2oI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "67002805-cd22-4543-fc50-37ce1ec4c678"
      },
      "source": [
        "#===============================================================================\n",
        "# Text Data & label preprocess for the model\n",
        "#===============================================================================\n",
        "\n",
        "print(len(df))\n",
        "idx_to_remove = []\n",
        "for i in range(len(df)):\n",
        "  if type(df['ds_summary'][i]) == float:\n",
        "    idx_to_remove.append(i)\n",
        "\n",
        "df = df.iloc[list(set(df.index) - set(idx_to_remove))]\n",
        "df.index = np.arange(0, len(df))\n",
        "print(len(df))\n",
        "\n",
        "df['importance'] = df['score']\n",
        "\n",
        "#score by daumsoft:\n",
        "#sentiment: 3 - positive, 2 - neutral, 1 - negative\n",
        "#score: 3 - important, 2 - normal, 1 - negligible \n",
        "for i in range(len(df)):\n",
        "  if df.loc[i,'sentiment'] == 3:\n",
        "    df.loc[i,'sentiment'] = 2\n",
        "  elif df.loc[i,'sentiment'] == 2:\n",
        "    df.loc[i,'sentiment'] = 1\n",
        "  elif df.loc[i,'sentiment'] == 1:\n",
        "    df.loc[i,'sentiment'] = 0\n",
        "  if df.loc[i, 'score'] == 1:\n",
        "    df.loc[i, 'importance'] = 0\n",
        "  elif df.loc[i, 'importance'] == 2:\n",
        "    df.loc[i, 'importance'] = 1\n",
        "  elif df.loc[i, 'importance'] == 3:\n",
        "    df.loc[i, 'importance'] = 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6584\n",
            "6584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlDfHbzxos3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "904a9733-2cb0-485d-cbe6-ae4a91a15c00"
      },
      "source": [
        "#===============================================================================\n",
        "# Prepare text data (Title + summary) for the model\n",
        "#===============================================================================\n",
        "df['title_summary'] = df['title'] + ' ' + df['ds_summary']\n",
        "title_summary_len = [len(df['title_summary'][i].split(' ')) for i in range(len(df))]\n",
        "df_stat = pd.DataFrame(title_summary_len)\n",
        "df_stat.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6584.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>57.095231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>30.884779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>46.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>76.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>181.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  6584.000000\n",
              "mean     57.095231\n",
              "std      30.884779\n",
              "min       9.000000\n",
              "25%      35.000000\n",
              "50%      46.000000\n",
              "75%      76.000000\n",
              "max     181.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkHvxOK3xpZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cabe7f07-1057-4790-c3ee-03738e4e0e1c"
      },
      "source": [
        "#===============================================================================\n",
        "# Data Preprocess & Distribution of labels (sentiment & importance)\n",
        "#===============================================================================\n",
        "X, y1, y2 = df['title_summary'], df['sentiment'], df['importance']\n",
        "y1_values, y2_values = y1.values, y2.values\n",
        "\n",
        "label_count_1, label_count_2 = [0,0,0], [0,0,0]\n",
        "for i in range(len(y1)):\n",
        "  label_count_1[y1_values[i]] += 1\n",
        "  label_count_2[y2_values[i]] += 1\n",
        "\n",
        "print(label_count_1, label_count_2)\n",
        "print([round(label_count_1[i]/sum(label_count_1),3) for i in range(len(label_count_1))], [round(label_count_2[i]/sum(label_count_2),3) for i in range(len(label_count_2))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1316, 3245, 2023] [2045, 2434, 2105]\n",
            "[0.2, 0.493, 0.307] [0.311, 0.37, 0.32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B84xcpiv8y5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# In order to fine-tune BERT for the importance classification\n",
        "#===============================================================================\n",
        "X, y1, y2 = df['title_summary'], df['importance'], df['importance']\n",
        "y1_values, y2_values = y1.values, y2.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0kaPYysxrLg",
        "colab_type": "text"
      },
      "source": [
        "## 2. Tokenization & Preprocess for model <a class=\"anchor\" id=\"section_2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nshl_ZiIxp-H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "e01cfb91a2f94a3ab66a0c5b0d4f965b",
            "6d27329495704389ac07370cef39f4b4",
            "712e8d1ea47546ccb4f2e471701f69c0",
            "316ea6cd3065459d861c63668872fb35",
            "52ee244311284f4fa4715d92b0d79b6e",
            "329f52a71c7e47a19324f8bb7b99b677",
            "04e9cca30b2747ff893829b53f3c0e72",
            "a0709537a6644c1f9d3a78d7dca96175"
          ]
        },
        "outputId": "722ba770-bf0d-4d57-e031-4f9a3a6bc9da"
      },
      "source": [
        "#===============================================================================\n",
        "# Download BERT tokenizer from huggingface\n",
        "#===============================================================================\n",
        "from transformers import *\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', X[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(X[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e01cfb91a2f94a3ab66a0c5b0d4f965b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Original:  Google Helping Mobile Publishing? Some Publishers Are Not So Sure Google said that it had designed AMP to prioritize speed and that it wanted to help — not harm — publishers, who get full accounting of traffic, data and advertising revenue. Google said serving up articles from its own internet network was the best way it knew to achieve the AMP speeds, which are as much as four times faster than a regular mobile web page. Google started AMP in 2015 because it worried that competitors like Facebook were drawing web surfers inside their networks with faster-loading articles and keeping them there.\n",
            "Tokenized:  ['google', 'helping', 'mobile', 'publishing', '?', 'some', 'publishers', 'are', 'not', 'so', 'sure', 'google', 'said', 'that', 'it', 'had', 'designed', 'amp', 'to', 'prior', '##iti', '##ze', 'speed', 'and', 'that', 'it', 'wanted', 'to', 'help', '—', 'not', 'harm', '—', 'publishers', ',', 'who', 'get', 'full', 'accounting', 'of', 'traffic', ',', 'data', 'and', 'advertising', 'revenue', '.', 'google', 'said', 'serving', 'up', 'articles', 'from', 'its', 'own', 'internet', 'network', 'was', 'the', 'best', 'way', 'it', 'knew', 'to', 'achieve', 'the', 'amp', 'speeds', ',', 'which', 'are', 'as', 'much', 'as', 'four', 'times', 'faster', 'than', 'a', 'regular', 'mobile', 'web', 'page', '.', 'google', 'started', 'amp', 'in', '2015', 'because', 'it', 'worried', 'that', 'competitors', 'like', 'facebook', 'were', 'drawing', 'web', 'surfer', '##s', 'inside', 'their', 'networks', 'with', 'faster', '-', 'loading', 'articles', 'and', 'keeping', 'them', 'there', '.']\n",
            "Token IDs:  [8224, 5094, 4684, 4640, 1029, 2070, 8544, 2024, 2025, 2061, 2469, 8224, 2056, 2008, 2009, 2018, 2881, 23713, 2000, 3188, 25090, 4371, 3177, 1998, 2008, 2009, 2359, 2000, 2393, 1517, 2025, 7386, 1517, 8544, 1010, 2040, 2131, 2440, 9529, 1997, 4026, 1010, 2951, 1998, 6475, 6599, 1012, 8224, 2056, 3529, 2039, 4790, 2013, 2049, 2219, 4274, 2897, 2001, 1996, 2190, 2126, 2009, 2354, 2000, 6162, 1996, 23713, 10898, 1010, 2029, 2024, 2004, 2172, 2004, 2176, 2335, 5514, 2084, 1037, 3180, 4684, 4773, 3931, 1012, 8224, 2318, 23713, 1999, 2325, 2138, 2009, 5191, 2008, 10159, 2066, 9130, 2020, 5059, 4773, 27747, 2015, 2503, 2037, 6125, 2007, 5514, 1011, 10578, 4790, 1998, 4363, 2068, 2045, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSdWN6Wxt_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "ef7c91d2-2acf-432c-bb17-18c6317e5817"
      },
      "source": [
        "#===============================================================================\n",
        "# In order to decide the MAX_LEN for padding and truncation purpose, check the \n",
        "# distribution of length of text data\n",
        "#===============================================================================\n",
        "\n",
        "\n",
        "max_len = 0\n",
        "too_big_input = []\n",
        "len_dist = []\n",
        "for i in range(len(X)):\n",
        "  input_ids = tokenizer.encode(X[i], add_special_tokens = True)\n",
        "  max_len = max(len(input_ids), max_len)\n",
        "  len_dist.append(len(input_ids))\n",
        "  if len(input_ids) > 512:\n",
        "    too_big_input.append(i)\n",
        "\n",
        "print('Max length of title is ', max_len)\n",
        "df_stat = pd.DataFrame(len_dist)\n",
        "df_stat.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length of title is  240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6584.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>76.385024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>39.503079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>16.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>48.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>63.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>103.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>240.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0\n",
              "count  6584.000000\n",
              "mean     76.385024\n",
              "std      39.503079\n",
              "min      16.000000\n",
              "25%      48.000000\n",
              "50%      63.000000\n",
              "75%     103.000000\n",
              "max     240.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRf68zY1xuox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# The preprocess code for the BERT\n",
        "# The following code is from https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "#===============================================================================\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation = True\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3A3DOgAx_cp",
        "colab_type": "text"
      },
      "source": [
        "## 3. Fine-Tuning Function <a class=\"anchor\" id=\"section_3\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQcyv_PMxwov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d0378e89-5217-4b7d-b43c-00d84042b2fa"
      },
      "source": [
        "#===============================================================================\n",
        "# Declare the BERT Classifier for later train and test purpose\n",
        "# if one wants to change the model, one can modify the below section\n",
        "#===============================================================================\n",
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertRNNClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bert Model for Classification Tasks\n",
        "    \"\"\"\n",
        "    def __init__(self,  hidden_size, num_layers, num_classes, input_size = 768, sm_hidden_size = 256,  freeze_bert=False,layers_to_freeze = []):\n",
        "      \"\"\"\n",
        "      @param    bert: a BertModel object\n",
        "      @param    rnn: LSTM layers\n",
        "        hidden_size: number of features in the hidden state h\n",
        "        num_layers: Number of recurrent layers\n",
        "        batch_first: If True, then the input and output tensors are provided as (batch, seq, feature)\n",
        "      @param    classifier: fc layer for the classification\n",
        "      @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "      \"\"\"\n",
        "      super(BertRNNClassifier, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "      self.input_size = input_size\n",
        "      self.sm_hidden_size = sm_hidden_size\n",
        "      D_out = num_classes\n",
        "      H = 100\n",
        "      # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "      #D_in, H, D_out = 768, 100, 3\n",
        "\n",
        "      # Instantiate BERT model\n",
        "      self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "      #RNN variables\n",
        "      self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
        "      '''\n",
        "      lstm output: output, (hn, cn) = lstm(input)\n",
        "      '''\n",
        "      \n",
        "      self.sm_fc = nn.Linear(input_size + hidden_size *2, sm_hidden_size)\n",
        "      self.fc = nn.Linear(sm_hidden_size, D_out)\n",
        "\n",
        "      # Instantiate an one-layer feed-forward classifier\n",
        "      \n",
        "\n",
        "      # Freeze the BERT model\n",
        "      if freeze_bert:\n",
        "          for param in self.bert.parameters():\n",
        "              param.requires_grad = False\n",
        "      else:\n",
        "        if layers_to_freeze != []:\n",
        "          for i in layers_to_freeze:\n",
        "            for param in self.bert.encoder.layer[i].parameters():\n",
        "              param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "\n",
        "        #num_layers * num_directions, batch, hidden_size\n",
        "        h0 = Variable(torch.zeros(self.num_layers*2, outputs[0].size()[0], self.hidden_size)).to(device)\n",
        "        c0 = Variable(torch.zeros(self.num_layers*2, outputs[0].size()[0], self.hidden_size)).to(device)\n",
        "\n",
        "        h0 = (nn.init.xavier_normal_(h0))\n",
        "        c0 = (nn.init.xavier_normal_(c0))\n",
        "\n",
        "        # Feed input (BERT embedded vector) to LSTM\n",
        "        lstm_output = self.lstm(outputs[0], (h0, c0))\n",
        "        out = torch.cat((lstm_output[0], outputs[0]), 2)\n",
        "\n",
        "        y2 = torch.tanh(self.sm_fc(out))\n",
        "        y2 = y2.unsqueeze(1)\n",
        "\n",
        "        y3 = F.max_pool2d(y2, (y2.size(2),1)).squeeze()\n",
        "\n",
        "        y4 = self.fc(y3)\n",
        "        final_out = F.softmax(y4, dim =-1)\n",
        "\n",
        "        return final_out\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 65 µs, sys: 1 µs, total: 66 µs\n",
            "Wall time: 73.2 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8DfgT7RyFBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Initialize model for the later train purpose\n",
        "#===============================================================================\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4, layers_to_freeze = []):\n",
        "    \"\"\"\n",
        "    Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "    # Instantiate Bert Classifier\n",
        "    hidden_size = 128\n",
        "    num_layers = 2\n",
        "    num_classes = 3\n",
        "    bert_classifier = BertRNNClassifier(hidden_size= hidden_size, num_layers= num_layers, num_classes= num_classes, freeze_bert = False, layers_to_freeze = layers_to_freeze)\n",
        "\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=6e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isZUWdF6yTJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# set_seed for reproducibility \n",
        "#===============================================================================\n",
        "import random\n",
        "import time\n",
        "from torch import nn\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezrfNB9_yG8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Train & Evaluate function \n",
        "#===============================================================================\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    #return float(val_loss), float(val_accuracy)\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFiTK90_yWO6",
        "colab_type": "text"
      },
      "source": [
        "## 4. Fine-Tuning <a class=\"anchor\" id=\"section_4\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB8jiM_myUBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8f74421d08f748d9a3bae9ce387c0c53",
            "818518f7363a4fd79be0c23d52f08b71",
            "6c18dbb95c514e80901b92446c04c16d",
            "9ef755b4f5744da0a1e3bdbbb6abad84",
            "8473a95bd8cd44d1ac88d8e583c1a51a",
            "72fe19108e974fbcaea52f547e17a249",
            "437b62d6119f462bb23c1fb0efce58ed",
            "41f4d469b53a4979a623223fd4ed57ba",
            "b975a7ed3e554f6392314a65deff0f2b",
            "f36fb454159342dfbcf05e8a9b17a82d",
            "72e8dab49abe447c800db6fbbf3b87cc",
            "f05e910647f649dab0f12e343da8ce38",
            "dcf0533513894c43bed4c507a603db71",
            "419c974d4cc84d47bce7855daa72d442",
            "1bcd1d7f577543e5a09c3486ee9de212",
            "ea4bebd2196b42fcab27912a7afebc37"
          ]
        },
        "outputId": "bfe45c41-7772-4a60-a387-86c4a04e5e23"
      },
      "source": [
        "#===============================================================================\n",
        "# Part where we train using the 5-fold cv\n",
        "#===============================================================================\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "#train and val are indices\n",
        "kf = KFold(n_splits=5, shuffle = True, random_state = 42)\n",
        "\n",
        "batch_size = 16\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "y1 = y1.astype(int)\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "  #Data Preparation\n",
        "  X_train = X[train_index]\n",
        "  X_val = X[val_index]\n",
        "  y1_train = y1[train_index]\n",
        "  y1_val = y1[val_index]\n",
        "  \n",
        "  train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "  val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "  train_labels = torch.tensor(y1_train.values)\n",
        "  val_labels = torch.tensor(y1_val.values)\n",
        "  \n",
        "  #Data Loader Class\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  val_sampler = RandomSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)\n",
        "  \n",
        "  #Fine Tune and Evaluation\n",
        "  set_seed(42)    # Set seed for reproducibility\n",
        "  bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "  val_loss1, val_accuracy1 = train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)\n",
        "  \n",
        "  val_loss.append(val_loss1)\n",
        "  val_accuracy.append(val_accuracy1)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f74421d08f748d9a3bae9ce387c0c53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b975a7ed3e554f6392314a65deff0f2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   1.058930   |     -      |     -     |   8.39   \n",
            "   1    |   40    |   1.022785   |     -      |     -     |   7.70   \n",
            "   1    |   60    |   1.014745   |     -      |     -     |   7.79   \n",
            "   1    |   80    |   0.970607   |     -      |     -     |   7.86   \n",
            "   1    |   100   |   0.959174   |     -      |     -     |   7.93   \n",
            "   1    |   120   |   0.931443   |     -      |     -     |   8.01   \n",
            "   1    |   140   |   0.906769   |     -      |     -     |   8.09   \n",
            "   1    |   160   |   0.931512   |     -      |     -     |   8.13   \n",
            "   1    |   180   |   0.960655   |     -      |     -     |   8.09   \n",
            "   1    |   200   |   0.946839   |     -      |     -     |   8.15   \n",
            "   1    |   220   |   0.935748   |     -      |     -     |   8.20   \n",
            "   1    |   240   |   0.936665   |     -      |     -     |   8.26   \n",
            "   1    |   260   |   0.887780   |     -      |     -     |   8.30   \n",
            "   1    |   280   |   0.908674   |     -      |     -     |   8.30   \n",
            "   1    |   300   |   0.893841   |     -      |     -     |   8.31   \n",
            "   1    |   320   |   0.914497   |     -      |     -     |   8.34   \n",
            "   1    |   329   |   0.939743   |     -      |     -     |   3.49   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.948878   |  0.881168  |   66.91   |  144.55  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.810832   |     -      |     -     |   8.88   \n",
            "   2    |   40    |   0.820910   |     -      |     -     |   8.56   \n",
            "   2    |   60    |   0.828072   |     -      |     -     |   8.62   \n",
            "   2    |   80    |   0.813838   |     -      |     -     |   8.58   \n",
            "   2    |   100   |   0.827827   |     -      |     -     |   8.55   \n",
            "   2    |   120   |   0.780761   |     -      |     -     |   8.55   \n",
            "   2    |   140   |   0.808743   |     -      |     -     |   8.52   \n",
            "   2    |   160   |   0.816714   |     -      |     -     |   8.57   \n",
            "   2    |   180   |   0.827757   |     -      |     -     |   8.61   \n",
            "   2    |   200   |   0.865843   |     -      |     -     |   8.66   \n",
            "   2    |   220   |   0.827385   |     -      |     -     |   8.73   \n",
            "   2    |   240   |   0.807918   |     -      |     -     |   8.73   \n",
            "   2    |   260   |   0.819468   |     -      |     -     |   8.72   \n",
            "   2    |   280   |   0.814593   |     -      |     -     |   8.70   \n",
            "   2    |   300   |   0.829924   |     -      |     -     |   8.72   \n",
            "   2    |   320   |   0.800782   |     -      |     -     |   8.72   \n",
            "   2    |   329   |   0.865277   |     -      |     -     |   3.62   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.820078   |  0.884921  |   65.36   |  153.88  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiiABOetyXwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b4051cb7-6830-494f-ffe7-cf690af515a3"
      },
      "source": [
        "#===============================================================================\n",
        "# Print the mean loss and accuracy of the 5-fold cv\n",
        "#===============================================================================\n",
        "print('The mean validation accuracy of 5-fold cv is: ',np.mean(val_accuracy))\n",
        "print('The mean validation loss of 5-fold cv is: ', np.mean(val_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean validation accuracy of 5-fold cv is:  65.36144578313252\n",
            "The mean validation loss of 5-fold cv is:  0.8849206236471613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmtUSifyZmz",
        "colab_type": "text"
      },
      "source": [
        "## 5. Confusion matrix & Critical Error <a class=\"anchor\" id=\"section_5\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7rijYf-yZLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Prediction function given the fine-tuned model\n",
        "#===============================================================================\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    val_accuracy = []\n",
        "    all_pred = []\n",
        "    all_real = []\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        all_real.append(b_labels)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        all_pred.append(preds)\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    all_real = torch.cat(all_real)\n",
        "    all_pred = torch.cat(all_pred)\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    #val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_accuracy,all_real, all_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIgagVMwyozJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Import py file for confusion matrtix\n",
        "#===============================================================================\n",
        "import sys\n",
        "import os\n",
        "py_file_location ='/content/drive/My Drive/Lib'\n",
        "sys.path.append(py_file_location)\n",
        "\n",
        "from confusion_matrix import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRc-RXknypf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6cc3ac3a-918f-40ae-b8dd-e3a0ac7b7f01"
      },
      "source": [
        "#===============================================================================\n",
        "# Make prediction for confusion matrix and critical error\n",
        "#===============================================================================\n",
        "acc,all_real, all_pred = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "critical_error = 0\n",
        "crit_error_index = []\n",
        "hit = 0\n",
        "for i in range(len(all_real)):\n",
        "  if all_real[i] == all_pred[i]:\n",
        "    hit += 1\n",
        "  if abs(all_pred[i] - all_real[i]) == 2:\n",
        "    critical_error += 1\n",
        "    crit_error_index.append(i)\n",
        "\n",
        "all_pred = all_pred.cpu().numpy()\n",
        "all_real = all_real.cpu().numpy()\n",
        "\n",
        "print('The accuracy of the model is :', hit/len(all_real))\n",
        "print('The critical error is : ',critical_error/len(all_real))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the model is : 0.6590736522399393\n",
            "The critical error is :  0.04100227790432802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8tMGlYNyqT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "6fb064fe-c4d6-43f9-e293-5879017937c1"
      },
      "source": [
        "#===============================================================================\n",
        "# Visualization of Confusion matrix\n",
        "#===============================================================================\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(all_real, all_pred)\n",
        "plot_confusion_matrix(cm,\n",
        "                      ['neg','neu', 'pos'],\n",
        "                      title='Confusion matrix',\n",
        "                      cmap=None,\n",
        "                      normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GC78q6j8SZO",
        "colab_type": "text"
      },
      "source": [
        "## Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxJGjr3H8UFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# import py for the EDA (Easy data Augmentation)\n",
        "# original code from https://github.com/jasonwei20/eda_nlp\n",
        "#===============================================================================\n",
        "import sys\n",
        "import os\n",
        "py_file_location ='/content/drive/My Drive/Lib'\n",
        "sys.path.append(py_file_location)\n",
        "\n",
        "#from easy_data_augmentation import *\n",
        "\n",
        "X, y1 = gen_eda(X, y1, alpha = 0.2, num_aug = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hqm9FO28WwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b289e1da-d320-4662-ad8b-2e8713d52294"
      },
      "source": [
        "#===============================================================================\n",
        "# Train for prediction\n",
        "#===============================================================================\n",
        "\n",
        "batch_size = 32\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_inputs, train_masks = preprocessing_for_bert(X)\n",
        "train_labels = torch.tensor(y1)\n",
        "#Data Loader Class\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
        "\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=6,layers_to_freeze= [0,1,2,3,4,5,6,7])\n",
        "train(bert_classifier, train_dataloader, epochs=6, evaluation=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   1.087555   |     -      |     -     |   24.06  \n",
            "   1    |   40    |   1.066501   |     -      |     -     |   22.77  \n",
            "   1    |   60    |   1.034840   |     -      |     -     |   22.69  \n",
            "   1    |   80    |   1.021303   |     -      |     -     |   22.63  \n",
            "   1    |   100   |   1.010791   |     -      |     -     |   22.61  \n",
            "   1    |   120   |   0.988113   |     -      |     -     |   22.66  \n",
            "   1    |   140   |   0.978539   |     -      |     -     |   22.64  \n",
            "   1    |   160   |   0.963898   |     -      |     -     |   22.63  \n",
            "   1    |   180   |   0.950281   |     -      |     -     |   22.62  \n",
            "   1    |   200   |   0.962042   |     -      |     -     |   22.66  \n",
            "   1    |   220   |   0.961325   |     -      |     -     |   22.62  \n",
            "   1    |   240   |   0.938153   |     -      |     -     |   22.63  \n",
            "   1    |   260   |   0.914050   |     -      |     -     |   22.62  \n",
            "   1    |   280   |   0.929804   |     -      |     -     |   22.61  \n",
            "   1    |   300   |   0.918766   |     -      |     -     |   22.69  \n",
            "   1    |   320   |   0.960607   |     -      |     -     |   22.79  \n",
            "   1    |   340   |   0.926551   |     -      |     -     |   22.90  \n",
            "   1    |   360   |   0.920862   |     -      |     -     |   22.95  \n",
            "   1    |   380   |   0.908724   |     -      |     -     |   22.96  \n",
            "   1    |   400   |   0.903429   |     -      |     -     |   22.96  \n",
            "   1    |   420   |   0.915084   |     -      |     -     |   22.86  \n",
            "   1    |   440   |   0.886501   |     -      |     -     |   22.81  \n",
            "   1    |   460   |   0.887360   |     -      |     -     |   22.74  \n",
            "   1    |   480   |   0.901408   |     -      |     -     |   22.74  \n",
            "   1    |   500   |   0.904927   |     -      |     -     |   22.71  \n",
            "   1    |   520   |   0.899531   |     -      |     -     |   22.78  \n",
            "   1    |   540   |   0.881408   |     -      |     -     |   22.74  \n",
            "   1    |   560   |   0.869350   |     -      |     -     |   22.77  \n",
            "   1    |   580   |   0.904630   |     -      |     -     |   22.77  \n",
            "   1    |   600   |   0.906175   |     -      |     -     |   22.77  \n",
            "   1    |   620   |   0.894038   |     -      |     -     |   22.76  \n",
            "   1    |   640   |   0.880603   |     -      |     -     |   22.72  \n",
            "   1    |   660   |   0.886410   |     -      |     -     |   22.77  \n",
            "   1    |   680   |   0.903921   |     -      |     -     |   22.78  \n",
            "   1    |   700   |   0.878634   |     -      |     -     |   22.80  \n",
            "   1    |   720   |   0.876052   |     -      |     -     |   22.75  \n",
            "   1    |   740   |   0.864346   |     -      |     -     |   22.76  \n",
            "   1    |   760   |   0.858202   |     -      |     -     |   22.78  \n",
            "   1    |   780   |   0.896649   |     -      |     -     |   22.77  \n",
            "   1    |   800   |   0.895367   |     -      |     -     |   22.76  \n",
            "   1    |   820   |   0.875489   |     -      |     -     |   22.77  \n",
            "   1    |   840   |   0.854325   |     -      |     -     |   22.78  \n",
            "   1    |   860   |   0.873934   |     -      |     -     |   22.80  \n",
            "   1    |   880   |   0.852861   |     -      |     -     |   22.81  \n",
            "   1    |   900   |   0.853808   |     -      |     -     |   22.77  \n",
            "   1    |   920   |   0.845152   |     -      |     -     |   22.80  \n",
            "   1    |   940   |   0.877688   |     -      |     -     |   22.78  \n",
            "   1    |   960   |   0.872891   |     -      |     -     |   22.79  \n",
            "   1    |   980   |   0.860659   |     -      |     -     |   22.78  \n",
            "   1    |  1000   |   0.835534   |     -      |     -     |   22.79  \n",
            "   1    |  1020   |   0.813528   |     -      |     -     |   22.77  \n",
            "   1    |  1040   |   0.859057   |     -      |     -     |   22.76  \n",
            "   1    |  1060   |   0.811827   |     -      |     -     |   22.79  \n",
            "   1    |  1080   |   0.877669   |     -      |     -     |   22.76  \n",
            "   1    |  1100   |   0.870104   |     -      |     -     |   22.79  \n",
            "   1    |  1120   |   0.840507   |     -      |     -     |   22.77  \n",
            "   1    |  1140   |   0.836739   |     -      |     -     |   22.83  \n",
            "   1    |  1160   |   0.857323   |     -      |     -     |   22.90  \n",
            "   1    |  1180   |   0.843928   |     -      |     -     |   22.93  \n",
            "   1    |  1200   |   0.865017   |     -      |     -     |   22.93  \n",
            "   1    |  1220   |   0.803516   |     -      |     -     |   22.88  \n",
            "   1    |  1240   |   0.826466   |     -      |     -     |   22.81  \n",
            "   1    |  1260   |   0.806061   |     -      |     -     |   22.77  \n",
            "   1    |  1280   |   0.826918   |     -      |     -     |   22.75  \n",
            "   1    |  1300   |   0.832593   |     -      |     -     |   22.76  \n",
            "   1    |  1320   |   0.837813   |     -      |     -     |   22.79  \n",
            "   1    |  1340   |   0.828277   |     -      |     -     |   22.77  \n",
            "   1    |  1360   |   0.805284   |     -      |     -     |   22.76  \n",
            "   1    |  1380   |   0.812778   |     -      |     -     |   22.80  \n",
            "   1    |  1400   |   0.802685   |     -      |     -     |   22.79  \n",
            "   1    |  1420   |   0.823291   |     -      |     -     |   22.74  \n",
            "   1    |  1440   |   0.826423   |     -      |     -     |   22.04  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.801708   |     -      |     -     |   23.91  \n",
            "   2    |   40    |   0.780612   |     -      |     -     |   22.76  \n",
            "   2    |   60    |   0.792987   |     -      |     -     |   22.79  \n",
            "   2    |   80    |   0.805058   |     -      |     -     |   22.77  \n",
            "   2    |   100   |   0.775029   |     -      |     -     |   22.81  \n",
            "   2    |   120   |   0.772668   |     -      |     -     |   22.80  \n",
            "   2    |   140   |   0.781368   |     -      |     -     |   22.79  \n",
            "   2    |   160   |   0.759649   |     -      |     -     |   22.79  \n",
            "   2    |   180   |   0.767753   |     -      |     -     |   22.78  \n",
            "   2    |   200   |   0.753166   |     -      |     -     |   22.78  \n",
            "   2    |   220   |   0.796570   |     -      |     -     |   22.78  \n",
            "   2    |   240   |   0.790913   |     -      |     -     |   22.79  \n",
            "   2    |   260   |   0.778576   |     -      |     -     |   22.78  \n",
            "   2    |   280   |   0.768461   |     -      |     -     |   22.83  \n",
            "   2    |   300   |   0.772170   |     -      |     -     |   22.91  \n",
            "   2    |   320   |   0.762208   |     -      |     -     |   22.91  \n",
            "   2    |   340   |   0.770715   |     -      |     -     |   22.92  \n",
            "   2    |   360   |   0.756420   |     -      |     -     |   22.95  \n",
            "   2    |   380   |   0.749044   |     -      |     -     |   22.85  \n",
            "   2    |   400   |   0.762197   |     -      |     -     |   22.77  \n",
            "   2    |   420   |   0.758652   |     -      |     -     |   22.75  \n",
            "   2    |   440   |   0.761786   |     -      |     -     |   22.76  \n",
            "   2    |   460   |   0.754646   |     -      |     -     |   22.81  \n",
            "   2    |   480   |   0.774095   |     -      |     -     |   22.93  \n",
            "   2    |   500   |   0.755060   |     -      |     -     |   22.88  \n",
            "   2    |   520   |   0.758521   |     -      |     -     |   22.97  \n",
            "   2    |   540   |   0.753866   |     -      |     -     |   22.90  \n",
            "   2    |   560   |   0.750951   |     -      |     -     |   22.81  \n",
            "   2    |   580   |   0.764012   |     -      |     -     |   22.75  \n",
            "   2    |   600   |   0.777205   |     -      |     -     |   22.75  \n",
            "   2    |   620   |   0.774138   |     -      |     -     |   22.77  \n",
            "   2    |   640   |   0.742558   |     -      |     -     |   22.77  \n",
            "   2    |   660   |   0.749319   |     -      |     -     |   22.77  \n",
            "   2    |   680   |   0.813407   |     -      |     -     |   22.78  \n",
            "   2    |   700   |   0.749259   |     -      |     -     |   22.77  \n",
            "   2    |   720   |   0.738926   |     -      |     -     |   22.79  \n",
            "   2    |   740   |   0.757048   |     -      |     -     |   22.77  \n",
            "   2    |   760   |   0.752473   |     -      |     -     |   22.75  \n",
            "   2    |   780   |   0.742470   |     -      |     -     |   22.78  \n",
            "   2    |   800   |   0.742791   |     -      |     -     |   22.75  \n",
            "   2    |   820   |   0.748232   |     -      |     -     |   22.77  \n",
            "   2    |   840   |   0.766119   |     -      |     -     |   22.79  \n",
            "   2    |   860   |   0.761255   |     -      |     -     |   22.79  \n",
            "   2    |   880   |   0.766030   |     -      |     -     |   22.76  \n",
            "   2    |   900   |   0.764300   |     -      |     -     |   22.78  \n",
            "   2    |   920   |   0.740075   |     -      |     -     |   22.78  \n",
            "   2    |   940   |   0.749279   |     -      |     -     |   22.79  \n",
            "   2    |   960   |   0.747291   |     -      |     -     |   22.77  \n",
            "   2    |   980   |   0.725775   |     -      |     -     |   22.79  \n",
            "   2    |  1000   |   0.745839   |     -      |     -     |   22.76  \n",
            "   2    |  1020   |   0.766595   |     -      |     -     |   22.76  \n",
            "   2    |  1040   |   0.764889   |     -      |     -     |   22.78  \n",
            "   2    |  1060   |   0.734869   |     -      |     -     |   22.76  \n",
            "   2    |  1080   |   0.733376   |     -      |     -     |   22.80  \n",
            "   2    |  1100   |   0.730781   |     -      |     -     |   22.74  \n",
            "   2    |  1120   |   0.733605   |     -      |     -     |   22.77  \n",
            "   2    |  1140   |   0.747558   |     -      |     -     |   22.77  \n",
            "   2    |  1160   |   0.757427   |     -      |     -     |   22.77  \n",
            "   2    |  1180   |   0.745598   |     -      |     -     |   22.75  \n",
            "   2    |  1200   |   0.749730   |     -      |     -     |   22.77  \n",
            "   2    |  1220   |   0.735253   |     -      |     -     |   22.77  \n",
            "   2    |  1240   |   0.749151   |     -      |     -     |   22.78  \n",
            "   2    |  1260   |   0.739900   |     -      |     -     |   22.79  \n",
            "   2    |  1280   |   0.749163   |     -      |     -     |   22.78  \n",
            "   2    |  1300   |   0.725731   |     -      |     -     |   22.80  \n",
            "   2    |  1320   |   0.738142   |     -      |     -     |   22.80  \n",
            "   2    |  1340   |   0.709890   |     -      |     -     |   22.78  \n",
            "   2    |  1360   |   0.748419   |     -      |     -     |   22.76  \n",
            "   2    |  1380   |   0.729245   |     -      |     -     |   22.76  \n",
            "   2    |  1400   |   0.716400   |     -      |     -     |   22.78  \n",
            "   2    |  1420   |   0.717985   |     -      |     -     |   22.75  \n",
            "   2    |  1440   |   0.726594   |     -      |     -     |   22.01  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.752742   |     -      |     -     |   23.89  \n",
            "   3    |   40    |   0.715367   |     -      |     -     |   22.80  \n",
            "   3    |   60    |   0.710233   |     -      |     -     |   22.75  \n",
            "   3    |   80    |   0.733139   |     -      |     -     |   22.75  \n",
            "   3    |   100   |   0.700506   |     -      |     -     |   22.77  \n",
            "   3    |   120   |   0.716272   |     -      |     -     |   22.74  \n",
            "   3    |   140   |   0.704100   |     -      |     -     |   22.77  \n",
            "   3    |   160   |   0.717545   |     -      |     -     |   22.76  \n",
            "   3    |   180   |   0.716142   |     -      |     -     |   22.76  \n",
            "   3    |   200   |   0.700913   |     -      |     -     |   22.76  \n",
            "   3    |   220   |   0.696312   |     -      |     -     |   22.75  \n",
            "   3    |   240   |   0.720365   |     -      |     -     |   22.71  \n",
            "   3    |   260   |   0.698684   |     -      |     -     |   22.77  \n",
            "   3    |   280   |   0.722899   |     -      |     -     |   22.76  \n",
            "   3    |   300   |   0.701781   |     -      |     -     |   22.75  \n",
            "   3    |   320   |   0.715054   |     -      |     -     |   22.75  \n",
            "   3    |   340   |   0.713923   |     -      |     -     |   22.74  \n",
            "   3    |   360   |   0.704765   |     -      |     -     |   22.74  \n",
            "   3    |   380   |   0.676513   |     -      |     -     |   22.76  \n",
            "   3    |   400   |   0.721161   |     -      |     -     |   22.76  \n",
            "   3    |   420   |   0.701525   |     -      |     -     |   22.76  \n",
            "   3    |   440   |   0.699924   |     -      |     -     |   22.72  \n",
            "   3    |   460   |   0.710674   |     -      |     -     |   22.77  \n",
            "   3    |   480   |   0.720759   |     -      |     -     |   22.74  \n",
            "   3    |   500   |   0.689376   |     -      |     -     |   22.77  \n",
            "   3    |   520   |   0.708285   |     -      |     -     |   22.81  \n",
            "   3    |   540   |   0.686656   |     -      |     -     |   22.73  \n",
            "   3    |   560   |   0.708189   |     -      |     -     |   22.77  \n",
            "   3    |   580   |   0.703366   |     -      |     -     |   22.76  \n",
            "   3    |   600   |   0.682698   |     -      |     -     |   22.76  \n",
            "   3    |   620   |   0.681648   |     -      |     -     |   22.76  \n",
            "   3    |   640   |   0.709407   |     -      |     -     |   22.81  \n",
            "   3    |   660   |   0.683758   |     -      |     -     |   22.76  \n",
            "   3    |   680   |   0.699840   |     -      |     -     |   22.77  \n",
            "   3    |   700   |   0.728037   |     -      |     -     |   22.77  \n",
            "   3    |   720   |   0.694523   |     -      |     -     |   22.75  \n",
            "   3    |   740   |   0.714910   |     -      |     -     |   22.75  \n",
            "   3    |   760   |   0.695511   |     -      |     -     |   22.77  \n",
            "   3    |   780   |   0.690072   |     -      |     -     |   22.77  \n",
            "   3    |   800   |   0.716938   |     -      |     -     |   22.75  \n",
            "   3    |   820   |   0.711011   |     -      |     -     |   22.78  \n",
            "   3    |   840   |   0.707322   |     -      |     -     |   22.80  \n",
            "   3    |   860   |   0.679664   |     -      |     -     |   22.70  \n",
            "   3    |   880   |   0.679188   |     -      |     -     |   22.64  \n",
            "   3    |   900   |   0.675482   |     -      |     -     |   22.62  \n",
            "   3    |   920   |   0.691294   |     -      |     -     |   22.59  \n",
            "   3    |   940   |   0.701786   |     -      |     -     |   22.60  \n",
            "   3    |   960   |   0.699616   |     -      |     -     |   22.64  \n",
            "   3    |   980   |   0.667972   |     -      |     -     |   22.59  \n",
            "   3    |  1000   |   0.682873   |     -      |     -     |   22.60  \n",
            "   3    |  1020   |   0.676958   |     -      |     -     |   22.66  \n",
            "   3    |  1040   |   0.701550   |     -      |     -     |   22.66  \n",
            "   3    |  1060   |   0.671060   |     -      |     -     |   22.61  \n",
            "   3    |  1080   |   0.685296   |     -      |     -     |   22.66  \n",
            "   3    |  1100   |   0.682051   |     -      |     -     |   22.60  \n",
            "   3    |  1120   |   0.672686   |     -      |     -     |   22.58  \n",
            "   3    |  1140   |   0.682621   |     -      |     -     |   22.58  \n",
            "   3    |  1160   |   0.666719   |     -      |     -     |   22.61  \n",
            "   3    |  1180   |   0.693120   |     -      |     -     |   22.58  \n",
            "   3    |  1200   |   0.685438   |     -      |     -     |   22.60  \n",
            "   3    |  1220   |   0.688486   |     -      |     -     |   22.57  \n",
            "   3    |  1240   |   0.702704   |     -      |     -     |   22.60  \n",
            "   3    |  1260   |   0.683889   |     -      |     -     |   22.56  \n",
            "   3    |  1280   |   0.698157   |     -      |     -     |   22.59  \n",
            "   3    |  1300   |   0.673052   |     -      |     -     |   22.59  \n",
            "   3    |  1320   |   0.674207   |     -      |     -     |   22.59  \n",
            "   3    |  1340   |   0.677077   |     -      |     -     |   22.61  \n",
            "   3    |  1360   |   0.696710   |     -      |     -     |   22.58  \n",
            "   3    |  1380   |   0.687398   |     -      |     -     |   22.61  \n",
            "   3    |  1400   |   0.700928   |     -      |     -     |   22.57  \n",
            "   3    |  1420   |   0.689491   |     -      |     -     |   22.60  \n",
            "   3    |  1440   |   0.703794   |     -      |     -     |   21.87  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.677293   |     -      |     -     |   23.76  \n",
            "   4    |   40    |   0.672462   |     -      |     -     |   22.61  \n",
            "   4    |   60    |   0.695498   |     -      |     -     |   22.58  \n",
            "   4    |   80    |   0.680924   |     -      |     -     |   22.63  \n",
            "   4    |   100   |   0.658213   |     -      |     -     |   22.57  \n",
            "   4    |   120   |   0.657404   |     -      |     -     |   22.60  \n",
            "   4    |   140   |   0.676398   |     -      |     -     |   22.62  \n",
            "   4    |   160   |   0.677554   |     -      |     -     |   22.60  \n",
            "   4    |   180   |   0.674490   |     -      |     -     |   22.58  \n",
            "   4    |   200   |   0.680195   |     -      |     -     |   22.60  \n",
            "   4    |   220   |   0.679310   |     -      |     -     |   22.59  \n",
            "   4    |   240   |   0.691029   |     -      |     -     |   22.60  \n",
            "   4    |   260   |   0.674603   |     -      |     -     |   22.59  \n",
            "   4    |   280   |   0.656745   |     -      |     -     |   22.61  \n",
            "   4    |   300   |   0.674897   |     -      |     -     |   22.59  \n",
            "   4    |   320   |   0.692125   |     -      |     -     |   22.62  \n",
            "   4    |   340   |   0.683429   |     -      |     -     |   22.64  \n",
            "   4    |   360   |   0.664191   |     -      |     -     |   22.58  \n",
            "   4    |   380   |   0.678571   |     -      |     -     |   22.59  \n",
            "   4    |   400   |   0.667244   |     -      |     -     |   22.61  \n",
            "   4    |   420   |   0.668774   |     -      |     -     |   22.61  \n",
            "   4    |   440   |   0.672919   |     -      |     -     |   22.57  \n",
            "   4    |   460   |   0.664546   |     -      |     -     |   22.60  \n",
            "   4    |   480   |   0.666544   |     -      |     -     |   22.61  \n",
            "   4    |   500   |   0.678272   |     -      |     -     |   22.59  \n",
            "   4    |   520   |   0.659089   |     -      |     -     |   22.60  \n",
            "   4    |   540   |   0.681829   |     -      |     -     |   22.58  \n",
            "   4    |   560   |   0.692742   |     -      |     -     |   22.63  \n",
            "   4    |   580   |   0.639512   |     -      |     -     |   22.58  \n",
            "   4    |   600   |   0.654228   |     -      |     -     |   22.61  \n",
            "   4    |   620   |   0.669440   |     -      |     -     |   22.56  \n",
            "   4    |   640   |   0.669410   |     -      |     -     |   22.64  \n",
            "   4    |   660   |   0.667981   |     -      |     -     |   22.58  \n",
            "   4    |   680   |   0.664314   |     -      |     -     |   22.59  \n",
            "   4    |   700   |   0.671941   |     -      |     -     |   22.59  \n",
            "   4    |   720   |   0.666880   |     -      |     -     |   22.60  \n",
            "   4    |   740   |   0.665053   |     -      |     -     |   22.57  \n",
            "   4    |   760   |   0.675129   |     -      |     -     |   22.61  \n",
            "   4    |   780   |   0.662979   |     -      |     -     |   22.62  \n",
            "   4    |   800   |   0.689399   |     -      |     -     |   22.60  \n",
            "   4    |   820   |   0.676459   |     -      |     -     |   22.57  \n",
            "   4    |   840   |   0.689128   |     -      |     -     |   22.63  \n",
            "   4    |   860   |   0.654430   |     -      |     -     |   22.57  \n",
            "   4    |   880   |   0.659521   |     -      |     -     |   22.61  \n",
            "   4    |   900   |   0.671851   |     -      |     -     |   22.58  \n",
            "   4    |   920   |   0.660100   |     -      |     -     |   22.61  \n",
            "   4    |   940   |   0.672858   |     -      |     -     |   22.61  \n",
            "   4    |   960   |   0.653838   |     -      |     -     |   22.59  \n",
            "   4    |   980   |   0.671796   |     -      |     -     |   22.62  \n",
            "   4    |  1000   |   0.679729   |     -      |     -     |   22.56  \n",
            "   4    |  1020   |   0.676862   |     -      |     -     |   22.57  \n",
            "   4    |  1040   |   0.674721   |     -      |     -     |   22.61  \n",
            "   4    |  1060   |   0.673709   |     -      |     -     |   22.62  \n",
            "   4    |  1080   |   0.658575   |     -      |     -     |   22.57  \n",
            "   4    |  1100   |   0.668257   |     -      |     -     |   22.61  \n",
            "   4    |  1120   |   0.654684   |     -      |     -     |   22.64  \n",
            "   4    |  1140   |   0.663859   |     -      |     -     |   22.60  \n",
            "   4    |  1160   |   0.690713   |     -      |     -     |   22.60  \n",
            "   4    |  1180   |   0.660803   |     -      |     -     |   22.57  \n",
            "   4    |  1200   |   0.662111   |     -      |     -     |   22.63  \n",
            "   4    |  1220   |   0.665562   |     -      |     -     |   22.58  \n",
            "   4    |  1240   |   0.658847   |     -      |     -     |   22.55  \n",
            "   4    |  1260   |   0.671888   |     -      |     -     |   22.58  \n",
            "   4    |  1280   |   0.674720   |     -      |     -     |   22.57  \n",
            "   4    |  1300   |   0.664290   |     -      |     -     |   22.55  \n",
            "   4    |  1320   |   0.664456   |     -      |     -     |   22.64  \n",
            "   4    |  1340   |   0.649782   |     -      |     -     |   22.71  \n",
            "   4    |  1360   |   0.668799   |     -      |     -     |   22.70  \n",
            "   4    |  1380   |   0.645607   |     -      |     -     |   22.61  \n",
            "   4    |  1400   |   0.653336   |     -      |     -     |   22.66  \n",
            "   4    |  1420   |   0.679250   |     -      |     -     |   22.64  \n",
            "   4    |  1440   |   0.670268   |     -      |     -     |   21.97  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.669619   |     -      |     -     |   23.78  \n",
            "   5    |   40    |   0.642326   |     -      |     -     |   22.70  \n",
            "   5    |   60    |   0.651860   |     -      |     -     |   22.62  \n",
            "   5    |   80    |   0.661111   |     -      |     -     |   22.70  \n",
            "   5    |   100   |   0.664119   |     -      |     -     |   22.77  \n",
            "   5    |   120   |   0.665817   |     -      |     -     |   22.75  \n",
            "   5    |   140   |   0.648554   |     -      |     -     |   22.68  \n",
            "   5    |   160   |   0.665087   |     -      |     -     |   22.60  \n",
            "   5    |   180   |   0.637485   |     -      |     -     |   22.59  \n",
            "   5    |   200   |   0.651640   |     -      |     -     |   22.63  \n",
            "   5    |   220   |   0.663095   |     -      |     -     |   22.56  \n",
            "   5    |   240   |   0.676175   |     -      |     -     |   22.63  \n",
            "   5    |   260   |   0.650893   |     -      |     -     |   22.64  \n",
            "   5    |   280   |   0.651464   |     -      |     -     |   22.67  \n",
            "   5    |   300   |   0.635784   |     -      |     -     |   22.61  \n",
            "   5    |   320   |   0.660191   |     -      |     -     |   22.59  \n",
            "   5    |   340   |   0.648271   |     -      |     -     |   22.54  \n",
            "   5    |   360   |   0.664897   |     -      |     -     |   22.55  \n",
            "   5    |   380   |   0.627538   |     -      |     -     |   22.54  \n",
            "   5    |   400   |   0.641920   |     -      |     -     |   22.53  \n",
            "   5    |   420   |   0.655385   |     -      |     -     |   22.61  \n",
            "   5    |   440   |   0.663037   |     -      |     -     |   22.58  \n",
            "   5    |   460   |   0.640526   |     -      |     -     |   22.59  \n",
            "   5    |   480   |   0.643534   |     -      |     -     |   22.59  \n",
            "   5    |   500   |   0.646949   |     -      |     -     |   22.58  \n",
            "   5    |   520   |   0.667891   |     -      |     -     |   22.57  \n",
            "   5    |   540   |   0.652409   |     -      |     -     |   22.58  \n",
            "   5    |   560   |   0.645474   |     -      |     -     |   22.60  \n",
            "   5    |   580   |   0.653996   |     -      |     -     |   22.60  \n",
            "   5    |   600   |   0.636180   |     -      |     -     |   22.61  \n",
            "   5    |   620   |   0.651509   |     -      |     -     |   22.59  \n",
            "   5    |   640   |   0.611758   |     -      |     -     |   22.60  \n",
            "   5    |   660   |   0.650668   |     -      |     -     |   22.58  \n",
            "   5    |   680   |   0.633214   |     -      |     -     |   22.57  \n",
            "   5    |   700   |   0.630330   |     -      |     -     |   22.57  \n",
            "   5    |   720   |   0.646529   |     -      |     -     |   22.58  \n",
            "   5    |   740   |   0.626537   |     -      |     -     |   22.59  \n",
            "   5    |   760   |   0.649581   |     -      |     -     |   22.60  \n",
            "   5    |   780   |   0.659131   |     -      |     -     |   22.61  \n",
            "   5    |   800   |   0.643433   |     -      |     -     |   22.60  \n",
            "   5    |   820   |   0.641784   |     -      |     -     |   22.57  \n",
            "   5    |   840   |   0.641922   |     -      |     -     |   22.60  \n",
            "   5    |   860   |   0.686943   |     -      |     -     |   22.60  \n",
            "   5    |   880   |   0.645283   |     -      |     -     |   22.55  \n",
            "   5    |   900   |   0.649327   |     -      |     -     |   22.60  \n",
            "   5    |   920   |   0.630141   |     -      |     -     |   22.56  \n",
            "   5    |   940   |   0.641448   |     -      |     -     |   22.60  \n",
            "   5    |   960   |   0.646229   |     -      |     -     |   22.60  \n",
            "   5    |   980   |   0.622282   |     -      |     -     |   22.61  \n",
            "   5    |  1000   |   0.634310   |     -      |     -     |   22.61  \n",
            "   5    |  1020   |   0.618969   |     -      |     -     |   22.60  \n",
            "   5    |  1040   |   0.635986   |     -      |     -     |   22.63  \n",
            "   5    |  1060   |   0.629525   |     -      |     -     |   22.58  \n",
            "   5    |  1080   |   0.658677   |     -      |     -     |   22.59  \n",
            "   5    |  1100   |   0.654351   |     -      |     -     |   22.58  \n",
            "   5    |  1120   |   0.637402   |     -      |     -     |   22.60  \n",
            "   5    |  1140   |   0.639003   |     -      |     -     |   22.60  \n",
            "   5    |  1160   |   0.653099   |     -      |     -     |   22.57  \n",
            "   5    |  1180   |   0.624811   |     -      |     -     |   22.60  \n",
            "   5    |  1200   |   0.634575   |     -      |     -     |   22.58  \n",
            "   5    |  1220   |   0.647585   |     -      |     -     |   22.57  \n",
            "   5    |  1240   |   0.635297   |     -      |     -     |   22.59  \n",
            "   5    |  1260   |   0.639360   |     -      |     -     |   22.60  \n",
            "   5    |  1280   |   0.645551   |     -      |     -     |   22.55  \n",
            "   5    |  1300   |   0.657664   |     -      |     -     |   22.56  \n",
            "   5    |  1320   |   0.662794   |     -      |     -     |   22.62  \n",
            "   5    |  1340   |   0.643736   |     -      |     -     |   22.64  \n",
            "   5    |  1360   |   0.660573   |     -      |     -     |   22.58  \n",
            "   5    |  1380   |   0.639113   |     -      |     -     |   22.61  \n",
            "   5    |  1400   |   0.626364   |     -      |     -     |   22.62  \n",
            "   5    |  1420   |   0.644319   |     -      |     -     |   22.60  \n",
            "   5    |  1440   |   0.628817   |     -      |     -     |   21.85  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   6    |   20    |   0.624425   |     -      |     -     |   23.70  \n",
            "   6    |   40    |   0.666270   |     -      |     -     |   22.56  \n",
            "   6    |   60    |   0.628545   |     -      |     -     |   22.57  \n",
            "   6    |   80    |   0.619303   |     -      |     -     |   22.58  \n",
            "   6    |   100   |   0.640327   |     -      |     -     |   22.57  \n",
            "   6    |   120   |   0.611752   |     -      |     -     |   22.55  \n",
            "   6    |   140   |   0.622672   |     -      |     -     |   22.55  \n",
            "   6    |   160   |   0.631813   |     -      |     -     |   22.56  \n",
            "   6    |   180   |   0.644007   |     -      |     -     |   22.60  \n",
            "   6    |   200   |   0.637947   |     -      |     -     |   22.56  \n",
            "   6    |   220   |   0.640598   |     -      |     -     |   22.57  \n",
            "   6    |   240   |   0.619397   |     -      |     -     |   22.56  \n",
            "   6    |   260   |   0.621845   |     -      |     -     |   22.60  \n",
            "   6    |   280   |   0.632469   |     -      |     -     |   22.51  \n",
            "   6    |   300   |   0.636333   |     -      |     -     |   22.60  \n",
            "   6    |   320   |   0.645483   |     -      |     -     |   22.57  \n",
            "   6    |   340   |   0.644832   |     -      |     -     |   22.57  \n",
            "   6    |   360   |   0.639941   |     -      |     -     |   22.56  \n",
            "   6    |   380   |   0.653246   |     -      |     -     |   22.54  \n",
            "   6    |   400   |   0.643418   |     -      |     -     |   22.55  \n",
            "   6    |   420   |   0.625587   |     -      |     -     |   22.56  \n",
            "   6    |   440   |   0.630260   |     -      |     -     |   22.61  \n",
            "   6    |   460   |   0.635951   |     -      |     -     |   22.55  \n",
            "   6    |   480   |   0.617083   |     -      |     -     |   22.55  \n",
            "   6    |   500   |   0.653938   |     -      |     -     |   22.57  \n",
            "   6    |   520   |   0.611156   |     -      |     -     |   22.58  \n",
            "   6    |   540   |   0.621595   |     -      |     -     |   22.57  \n",
            "   6    |   560   |   0.665717   |     -      |     -     |   22.54  \n",
            "   6    |   580   |   0.630195   |     -      |     -     |   22.54  \n",
            "   6    |   600   |   0.654691   |     -      |     -     |   22.59  \n",
            "   6    |   620   |   0.636994   |     -      |     -     |   22.57  \n",
            "   6    |   640   |   0.639687   |     -      |     -     |   22.62  \n",
            "   6    |   660   |   0.638377   |     -      |     -     |   22.56  \n",
            "   6    |   680   |   0.634020   |     -      |     -     |   22.58  \n",
            "   6    |   700   |   0.629018   |     -      |     -     |   22.54  \n",
            "   6    |   720   |   0.631455   |     -      |     -     |   22.61  \n",
            "   6    |   740   |   0.624168   |     -      |     -     |   22.57  \n",
            "   6    |   760   |   0.650516   |     -      |     -     |   22.55  \n",
            "   6    |   780   |   0.636516   |     -      |     -     |   22.57  \n",
            "   6    |   800   |   0.652675   |     -      |     -     |   22.55  \n",
            "   6    |   820   |   0.634479   |     -      |     -     |   22.59  \n",
            "   6    |   840   |   0.661429   |     -      |     -     |   22.61  \n",
            "   6    |   860   |   0.641051   |     -      |     -     |   22.56  \n",
            "   6    |   880   |   0.633773   |     -      |     -     |   22.58  \n",
            "   6    |   900   |   0.649764   |     -      |     -     |   22.59  \n",
            "   6    |   920   |   0.642221   |     -      |     -     |   22.57  \n",
            "   6    |   940   |   0.628474   |     -      |     -     |   22.56  \n",
            "   6    |   960   |   0.662334   |     -      |     -     |   22.56  \n",
            "   6    |   980   |   0.619984   |     -      |     -     |   22.58  \n",
            "   6    |  1000   |   0.617550   |     -      |     -     |   22.58  \n",
            "   6    |  1020   |   0.629518   |     -      |     -     |   22.59  \n",
            "   6    |  1040   |   0.642803   |     -      |     -     |   22.56  \n",
            "   6    |  1060   |   0.614329   |     -      |     -     |   22.58  \n",
            "   6    |  1080   |   0.645126   |     -      |     -     |   22.56  \n",
            "   6    |  1100   |   0.635517   |     -      |     -     |   22.55  \n",
            "   6    |  1120   |   0.625083   |     -      |     -     |   22.61  \n",
            "   6    |  1140   |   0.620532   |     -      |     -     |   22.57  \n",
            "   6    |  1160   |   0.637498   |     -      |     -     |   22.57  \n",
            "   6    |  1180   |   0.619921   |     -      |     -     |   22.59  \n",
            "   6    |  1200   |   0.622870   |     -      |     -     |   22.56  \n",
            "   6    |  1220   |   0.636799   |     -      |     -     |   22.56  \n",
            "   6    |  1240   |   0.610305   |     -      |     -     |   22.59  \n",
            "   6    |  1260   |   0.625990   |     -      |     -     |   22.58  \n",
            "   6    |  1280   |   0.634880   |     -      |     -     |   22.55  \n",
            "   6    |  1300   |   0.635579   |     -      |     -     |   22.58  \n",
            "   6    |  1320   |   0.643713   |     -      |     -     |   22.59  \n",
            "   6    |  1340   |   0.621424   |     -      |     -     |   22.56  \n",
            "   6    |  1360   |   0.656069   |     -      |     -     |   22.61  \n",
            "   6    |  1380   |   0.635005   |     -      |     -     |   22.58  \n",
            "   6    |  1400   |   0.627199   |     -      |     -     |   22.58  \n",
            "   6    |  1420   |   0.621640   |     -      |     -     |   22.57  \n",
            "   6    |  1440   |   0.637204   |     -      |     -     |   21.83  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9oALgll8Xy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Prediction function \n",
        "#===============================================================================\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    val_accuracy = []\n",
        "    all_pred = []\n",
        "    all_real = []\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        all_real.append(b_labels)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        all_pred.append(preds)\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    all_real = torch.cat(all_real)\n",
        "    all_pred = torch.cat(all_pred)\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    #val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_accuracy,all_real, all_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlY5b9_8d7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "be7a2553-bd13-4d7a-de0d-eac96422d598"
      },
      "source": [
        "#===============================================================================\n",
        "# Test the trained model\n",
        "#===============================================================================\n",
        "df_test = pd.read_json('/content/drive/My Drive/KIS data/test_data.json')\n",
        "\n",
        "print(len(df_test))\n",
        "idx_to_remove = []\n",
        "for i in range(len(df_test)):\n",
        "  if type(df_test['summary'][i]) == float:\n",
        "    idx_to_remove.append(i)\n",
        "  if type(df_test['title'][i]) == float:\n",
        "    idx_to_remove.append(i)\n",
        "\n",
        "df_test = df_test.iloc[list(set(df_test.index) - set(idx_to_remove))]\n",
        "df_test.index = np.arange(0, len(df_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_test['title_summary'] = df_test['title'] + ' ' + df_test['summary']\n",
        "df_test= df_test.drop(df_test[df_test['title_summary'].isnull()].index)\n",
        "df_test.index = np.arange(0, len(df_test))\n",
        "df_test= df_test.drop(df_test[df_test['importance'].isnull()].index)\n",
        "df_test.index = np.arange(0, len(df_test))\n",
        "df_test= df_test.drop(df_test[df_test['sentiment'].isnull()].index)\n",
        "df_test.index = np.arange(0, len(df_test))\n",
        "title_summary_len = [len(df_test['title_summary'][i].split(' ')) for i in range(len(df_test))]\n",
        "df_stat = pd.DataFrame(title_summary_len)\n",
        "df_stat.describe()\n",
        "print(len(df_test))\n",
        "\n",
        "X_test, y1_test, y2_test = df_test['title_summary'], df_test['sentiment'].apply(int), df_test['importance'].apply(int)\n",
        "y1_test_values, y2_test_values = y1_test.values, y2_test.values\n",
        "\n",
        "label_count_test_1, label_count_test_2 = [0,0,0], [0,0,0]\n",
        "for i in range(len(y1_test)):\n",
        "  label_count_test_1[y1_test_values[i]] += 1\n",
        "  label_count_test_2[y2_test_values[i]] += 1\n",
        "\n",
        "print(label_count_test_1, label_count_test_2)\n",
        "print([round(label_count_test_1[i]/sum(label_count_test_1),3) for i in range(len(label_count_test_1))], [round(label_count_test_2[i]/sum(label_count_test_2),3) for i in range(len(label_count_test_2))])\n",
        "\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "test_labels = torch.tensor(y1_test)\n",
        "#Data Loader Class\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)\n",
        "\n",
        "acc,all_real, all_pred = bert_predict(bert_classifier, test_dataloader)\n",
        "\n",
        "critical_error = 0\n",
        "crit_error_index = []\n",
        "hit = 0\n",
        "for i in range(len(all_real)):\n",
        "  if all_real[i] == all_pred[i]:\n",
        "    hit += 1\n",
        "  if abs(all_pred[i] - all_real[i]) == 2:\n",
        "    critical_error += 1\n",
        "    crit_error_index.append(i)\n",
        "\n",
        "all_pred = all_pred.cpu().numpy()\n",
        "all_real = all_real.cpu().numpy()\n",
        "\n",
        "print('The accuracy of the model is :', hit/len(all_real))\n",
        "print('The critical error is : ',critical_error/len(all_real))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2898\n",
            "2866\n",
            "[366, 1585, 915] [1052, 918, 896]\n",
            "[0.128, 0.553, 0.319] [0.367, 0.32, 0.313]\n",
            "The accuracy of the model is : 0.3217027215631542\n",
            "The critical error is :  0.15003489183531055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cph59Xhw8egL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "py_file_location ='/content/drive/My Drive/Lib'\n",
        "sys.path.append(py_file_location)\n",
        "\n",
        "from confusion_matrix import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7VIVo8T8f6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===============================================================================\n",
        "# Visualization of confusion matrix\n",
        "#===============================================================================\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(all_real, all_pred)\n",
        "plot_confusion_matrix(cm,\n",
        "                      ['neg','neu', 'pos'],\n",
        "                      title='Confusion matrix',\n",
        "                      cmap=None,\n",
        "                      normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}